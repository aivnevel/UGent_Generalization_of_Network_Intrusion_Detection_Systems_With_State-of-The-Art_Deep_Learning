{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LuNet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 6.5 MB/s eta 0:00:01       | 7.0 MB 6.5 MB/s eta 0:00:02     |███████████████████████▎        | 12.9 MB 6.5 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.18.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "DIR = './Datasets/local/'\n",
    "\n",
    "dfs = {}\n",
    "names = []\n",
    "for filename in os.listdir(DIR):\n",
    "    dfs[filename] = pd.read_feather(DIR + filename)\n",
    "    dfs[filename].columns = dfs[filename].columns.str.replace(' ', '_')\n",
    "    names.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Reshape, BatchNormalization, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(76, 1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization(axis=0))\n",
    "    model.add(Reshape((-1, 64), input_shape=(64,)))\n",
    "    model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization(axis=0))\n",
    "    model.add(Reshape((-1, 128), input_shape=(128,)))\n",
    "    model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(BatchNormalization(axis=0))\n",
    "    model.add(Reshape((-1, 256), input_shape=(256,)))\n",
    "    model.add(LSTM(256, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "('Input has undefined `axis` dimension. Input shape: ', TensorShape([None, 2368]))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-e7abdb0a1545>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.33\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'binary_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m156264\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m76\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m39067\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m76\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_delta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-54acc072281f>\u001b[0m in \u001b[0;36mcreate_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMaxPooling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpool_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tanh'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, layer)\u001b[0m\n\u001b[1;32m    211\u001b[0m       \u001b[0;31m# If the model is being built continuously on top of an input layer:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m       \u001b[0;31m# refresh its output.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m       \u001b[0moutput_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSINGLE_LAYER_OUTPUT_ERROR_MSG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m           \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m           \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m           \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2414\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2415\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2416\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2417\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2418\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/layers/normalization.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    374\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0maxis_to_dim\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         raise ValueError('Input has undefined `axis` dimension. Input shape: ',\n\u001b[0;32m--> 376\u001b[0;31m                          input_shape)\n\u001b[0m\u001b[1;32m    377\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis_to_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: ('Input has undefined `axis` dimension. Input shape: ', TensorShape([None, 2368]))"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataframe = dfs[\"cicdos2017\"]\n",
    "y = dataframe['label']\n",
    "scaler = MinMaxScaler()\n",
    "df = dataframe.drop(columns=['label'])\n",
    "X = scaler.fit_transform(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "model = create_model()\n",
    "model.compile(optimizer='Adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(X_train.reshape(156264,76,1), y_train, validation_data=(X_val.reshape(39067,76,1), y_val), epochs = 200, batch_size=2048, callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.0001)], verbose = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(X_test.reshape(96209,76,1), y_test, batch_size = 2048, verbose = 0)\n",
    "print('loss:', loss, ' accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2017_dos = [ \"Wednesday-workingHours-DoS\" ]\n",
    "df2017_portscan = [ \"Friday-WorkingHours-Afternoon-PortScan\" ]\n",
    "df2017_botnet = [ \"Friday-WorkingHours-Morning-Botnet\" ]\n",
    "df2017_infiltration = [ \"Thursday-WorkingHours-Afternoon-Infiltration\" ]\n",
    "df2017_webattacks = [ \"Thursday-WorkingHours-Morning-WebAttacks\" ]\n",
    "df2017_bruteforce = [ \"Tuesday-WorkingHours-Bruteforce\" ]\n",
    "df2017_ddos = [ \"Friday-WorkingHours-Afternoon-DDoS\" ]\n",
    "\n",
    "df2018_botnet = [ \"Friday-02-03-2018_TrafficForML_CICFlowMeter\" ]\n",
    "df2018_webattacks = [ \"Thursday-22-02-2018_TrafficForML_CICFlowMeter\",\n",
    "                     \"Friday-23-02-2018_TrafficForML_CICFlowMeter\" ]\n",
    "df2018_bruteforce = [ \"Wednesday-14-02-2018_TrafficForML_CICFlowMeter\" ]\n",
    "df2018_infiltration = [ \"Wednesday-28-02-2018_TrafficForML_CICFlowMeter\",\n",
    "                       \"Thursday-01-03-2018_TrafficForML_CICFlowMeter\" ]\n",
    "df2018_dos = [ \"Friday-16-02-2018_TrafficForML_CICFlowMeter\",\n",
    "                \"Thursday-15-02-2018_TrafficForML_CICFlowMeter\" ]\n",
    "df2018_ddos = [ \"Tuesday-20-02-2018_TrafficForML_CICFlowMeter\",\n",
    "               \"Wednesday-21-02-2018_TrafficForML_CICFlowMeter\",]\n",
    "\n",
    "df2019_ddos = [ \"01_12_DrDoS_DNS\", \"01_12_DrDoS_LDAP\", \"01_12_DrDoS_MSSQL\", \"01_12_DrDoS_NetBIOS\",\n",
    "                \"01_12_DrDoS_NTP\", \"01_12_DrDoS_SNMP\", \"01_12_DrDoS_SSDP\", \"01_12_DrDoS_UDP\",\n",
    "                \"01_12_Syn\", \"01_12_TFTP\", \"01_12_UDPLag\", \"03_11_LDAP\", \"03_11_MSSQL\",\n",
    "                \"03_11_NetBIOS\", \"03_11_Portmap\", \"03_11_Syn\", \"03_11_UDP\", \"03_11_UDPLag\" ]\n",
    "\n",
    "dfs_ddos = [ df2019_ddos, df2018_ddos, df2017_ddos]\n",
    "dfs_dos = [ df2018_dos, df2017_dos ]\n",
    "dfs_botnet = [ df2017_botnet, df2018_botnet ]\n",
    "dfs_infiltration = [ df2017_infiltration, df2018_infiltration]\n",
    "dfs_webattacks = [ df2017_webattacks, df2018_webattacks ]\n",
    "dfs_bruteforce = [ df2017_bruteforce, df2018_bruteforce ]\n",
    "dfs_names = [ dfs_ddos, dfs_dos, dfs_botnet, dfs_infiltration, dfs_webattacks, dfs_bruteforce ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_2019 = dfs[\"01_12_DrDoS_DNS\"]\n",
    "for dataset in df2019_ddos:\n",
    "    if dataset == \"01_12_DrDoS_DNS\":\n",
    "        continue;\n",
    "    dfs_2019.append(dfs[dataset])\n",
    "\n",
    "dfs_2018 = dfs[\"Tuesday-20-02-2018_TrafficForML_CICFlowMeter\"]\n",
    "for dataset in df2018_ddos:\n",
    "    if dataset == \"01_12_DrDoS_DNS\":\n",
    "        continue;\n",
    "dfs_2018.append(dfs[dataset])\n",
    "\n",
    "dfs_2017 = dfs[\"Friday-WorkingHours-Afternoon-DDoS\"]\n",
    "\n",
    "ddos_datasets = {}\n",
    "ddos_datasets[\"2017\"] = dfs_2017\n",
    "ddos_datasets[\"2018\"] = dfs_2018\n",
    "ddos_datasets[\"2019\"] = dfs_2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665.5\n"
     ]
    }
   ],
   "source": [
    "print(ddos_datasets[\"2019\"].shape[0]*0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset = ddos_datasets[\"2017\"]\n",
    "dataset.append(ddos_datasets[\"2018\"])\n",
    "#dataset.append(ddos_datasets[\"2018\"].head(int(ddos_datasets[\"2019\"].shape[0]*0.1)))\n",
    "y = dataset['label']\n",
    "scaler = MinMaxScaler()\n",
    "df = dataset.drop(columns=['label'])\n",
    "X = scaler.fit_transform(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "input_train_shape = X_train.shape\n",
    "input_test_shape = X_test.shape \n",
    "input_val_shape = X_val.shape\n",
    "input_shape = (input_train_shape[1], 1)\n",
    "# Reshape the training data to include channels\n",
    "input_train = X_train.reshape(input_train_shape[0], input_train_shape[1], 1)\n",
    "input_val = X_val.reshape(input_val_shape[0], input_val_shape[1], 1)\n",
    "input_test = X_test.reshape(input_test_shape[0], input_test_shape[1], 1)\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_val = input_val.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "# Normalize input data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "input_val = input_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.layers import LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Reshape, BatchNormalization, Flatten, GlobalAveragePooling1D\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "\n",
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((-1, 64), input_shape=(64,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((-1, 128), input_shape=(128,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((-1, 256), input_shape=(256,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(256, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='Adam', loss = 'binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/60 [==============================] - 62s 1s/step - loss: 0.4493 - accuracy: 0.7685 - val_loss: 0.7691 - val_accuracy: 0.4332\n",
      "Epoch 2/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0761 - accuracy: 0.9685 - val_loss: 0.8554 - val_accuracy: 0.4332\n",
      "Epoch 3/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0546 - accuracy: 0.9751 - val_loss: 1.2336 - val_accuracy: 0.4332\n",
      "Epoch 4/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0707 - accuracy: 0.9693 - val_loss: 3.4168 - val_accuracy: 0.4332\n",
      "Epoch 5/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0487 - accuracy: 0.9775 - val_loss: 6.2873 - val_accuracy: 0.4332\n",
      "Epoch 6/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0411 - accuracy: 0.9818 - val_loss: 7.2720 - val_accuracy: 0.4332\n",
      "Epoch 7/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0462 - accuracy: 0.9793 - val_loss: 7.4402 - val_accuracy: 0.4332\n",
      "Epoch 8/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0440 - accuracy: 0.9809 - val_loss: 8.8470 - val_accuracy: 0.4332\n",
      "Epoch 9/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0366 - accuracy: 0.9842 - val_loss: 7.2290 - val_accuracy: 0.4332\n",
      "Epoch 10/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0339 - accuracy: 0.9848 - val_loss: 7.9444 - val_accuracy: 0.4332\n",
      "Epoch 11/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0333 - accuracy: 0.9847 - val_loss: 6.2433 - val_accuracy: 0.4839\n",
      "Epoch 12/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0291 - accuracy: 0.9872 - val_loss: 7.3678 - val_accuracy: 0.4817\n",
      "Epoch 13/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0288 - accuracy: 0.9868 - val_loss: 1.8937 - val_accuracy: 0.6288\n",
      "Epoch 14/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0321 - accuracy: 0.9856 - val_loss: 0.0967 - val_accuracy: 0.9493\n",
      "Epoch 15/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0299 - accuracy: 0.9861 - val_loss: 0.1620 - val_accuracy: 0.9532\n",
      "Epoch 16/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0291 - accuracy: 0.9869 - val_loss: 2.6722 - val_accuracy: 0.5331\n",
      "Epoch 17/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0317 - accuracy: 0.9859 - val_loss: 2.7547 - val_accuracy: 0.4810\n",
      "Epoch 18/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0261 - accuracy: 0.9883 - val_loss: 0.2168 - val_accuracy: 0.9526\n",
      "Epoch 19/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0293 - accuracy: 0.9868 - val_loss: 6.4966 - val_accuracy: 0.4339\n",
      "Epoch 20/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0243 - accuracy: 0.9887 - val_loss: 6.6419 - val_accuracy: 0.4332\n",
      "Epoch 21/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0253 - accuracy: 0.9880 - val_loss: 0.0286 - val_accuracy: 0.9841\n",
      "Epoch 22/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0238 - accuracy: 0.9887 - val_loss: 0.0431 - val_accuracy: 0.9852\n",
      "Epoch 23/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0230 - accuracy: 0.9889 - val_loss: 0.2315 - val_accuracy: 0.9059\n",
      "Epoch 24/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0235 - accuracy: 0.9890 - val_loss: 1.0277 - val_accuracy: 0.7242\n",
      "Epoch 25/30\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.0241 - accuracy: 0.9888 - val_loss: 1.8859 - val_accuracy: 0.7792\n",
      "Epoch 26/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0245 - accuracy: 0.9888 - val_loss: 7.8947 - val_accuracy: 0.4332\n",
      "Epoch 27/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0229 - accuracy: 0.9891 - val_loss: 0.0859 - val_accuracy: 0.9815\n",
      "Epoch 28/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0229 - accuracy: 0.9893 - val_loss: 4.2227 - val_accuracy: 0.6137\n",
      "Epoch 29/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0227 - accuracy: 0.9889 - val_loss: 0.9013 - val_accuracy: 0.6188\n",
      "Epoch 30/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0223 - accuracy: 0.9896 - val_loss: 0.0199 - val_accuracy: 0.9902\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(input_train, y_train, validation_data=(input_val, y_val), epochs = 30, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.07%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(input_test, y_test, batch_size = 2048, verbose = 0)\n",
    "print(str(int(accuracy*10000)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = ddos_datasets[\"2019\"]\n",
    "y_true = dataset_test['label']\n",
    "y_true = y_true.to_list()\n",
    "scaler = MinMaxScaler()\n",
    "df = dataset_test.drop(columns=['label'])\n",
    "X = scaler.fit_transform(df)\n",
    "y = model.predict(X.reshape(6655,76,1))\n",
    "y_round = tf.make_ndarray(tf.make_tensor_proto(tf.round(y))).reshape([len(y)])\n",
    "y_round = list(y_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gebalanceerde accuracy van model met een ongekende dataset:\n",
      "41.64%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "print(\"Gebalanceerde accuracy van model met een ongekende dataset:\")\n",
    "print(str(int(balanced_accuracy_score(y_true, y_round)*10000)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "dataset = ddos_datasets[\"2017\"]\n",
    "dataset.append(ddos_datasets[\"2018\"])\n",
    "trainingdataset, testingdataset = train_test_split(ddos_datasets[\"2019\"], test_size=0.1)\n",
    "dataset.append(trainingdataset)\n",
    "y = dataset['label']\n",
    "scaler = MinMaxScaler()\n",
    "df = dataset.drop(columns=['label'])\n",
    "X = scaler.fit_transform(df)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2)\n",
    "\n",
    "input_train_shape = X_train.shape\n",
    "input_test_shape = X_test.shape \n",
    "input_val_shape = X_val.shape\n",
    "input_shape = (input_train_shape[1], 1)\n",
    "# Reshape the training data to include channels\n",
    "input_train = X_train.reshape(input_train_shape[0], input_train_shape[1], 1)\n",
    "input_val = X_val.reshape(input_val_shape[0], input_val_shape[1], 1)\n",
    "input_test = X_test.reshape(input_test_shape[0], input_test_shape[1], 1)\n",
    "# Parse numbers as floats\n",
    "input_train = input_train.astype('float32')\n",
    "input_val = input_val.astype('float32')\n",
    "input_test = input_test.astype('float32')\n",
    "# Normalize input data\n",
    "input_train = input_train / 255\n",
    "input_test = input_test / 255\n",
    "input_val = input_val / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=input_shape))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((-1, 64), input_shape=(64,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(64, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(128))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((-1, 128), input_shape=(128,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(128, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=256, kernel_size=3, activation='relu'))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Reshape((-1, 256), input_shape=(256,)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(LSTM(256, return_sequences = True, activation='tanh'))\n",
    "    model.add(Dense(512))\n",
    "    model.add(Dropout(.5))\n",
    "    \n",
    "    model.add(Conv1D(filters=512, kernel_size=3, activation='relu'))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dropout(.5))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "60/60 [==============================] - 7s 113ms/step - loss: 0.3760 - accuracy: 0.8024 - val_loss: 0.6851 - val_accuracy: 0.5651\n",
      "Epoch 2/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0657 - accuracy: 0.9706 - val_loss: 0.7840 - val_accuracy: 0.4349\n",
      "Epoch 3/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0639 - accuracy: 0.9720 - val_loss: 1.7202 - val_accuracy: 0.4349\n",
      "Epoch 4/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0768 - accuracy: 0.9678 - val_loss: 4.1218 - val_accuracy: 0.4349\n",
      "Epoch 5/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0502 - accuracy: 0.9780 - val_loss: 4.1694 - val_accuracy: 0.4349\n",
      "Epoch 6/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0403 - accuracy: 0.9817 - val_loss: 3.7588 - val_accuracy: 0.4349\n",
      "Epoch 7/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0351 - accuracy: 0.9835 - val_loss: 6.4936 - val_accuracy: 0.4349\n",
      "Epoch 8/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0347 - accuracy: 0.9839 - val_loss: 5.9728 - val_accuracy: 0.4344\n",
      "Epoch 9/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0332 - accuracy: 0.9841 - val_loss: 10.7621 - val_accuracy: 0.4346\n",
      "Epoch 10/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0311 - accuracy: 0.9853 - val_loss: 11.2727 - val_accuracy: 0.4346\n",
      "Epoch 11/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0302 - accuracy: 0.9856 - val_loss: 8.8629 - val_accuracy: 0.5159\n",
      "Epoch 12/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0282 - accuracy: 0.9870 - val_loss: 0.2826 - val_accuracy: 0.8467\n",
      "Epoch 13/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0283 - accuracy: 0.9864 - val_loss: 6.0062 - val_accuracy: 0.4517\n",
      "Epoch 14/30\n",
      "60/60 [==============================] - 5s 79ms/step - loss: 0.0372 - accuracy: 0.9821 - val_loss: 13.2972 - val_accuracy: 0.4182\n",
      "Epoch 15/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0299 - accuracy: 0.9855 - val_loss: 17.2727 - val_accuracy: 0.4256\n",
      "Epoch 16/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0327 - accuracy: 0.9853 - val_loss: 6.4972 - val_accuracy: 0.4349\n",
      "Epoch 17/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0315 - accuracy: 0.9858 - val_loss: 10.2381 - val_accuracy: 0.4350\n",
      "Epoch 18/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0262 - accuracy: 0.9873 - val_loss: 7.7259 - val_accuracy: 0.5308\n",
      "Epoch 19/30\n",
      "60/60 [==============================] - 5s 76ms/step - loss: 0.0251 - accuracy: 0.9881 - val_loss: 5.7442 - val_accuracy: 0.5404\n",
      "Epoch 20/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0277 - accuracy: 0.9871 - val_loss: 0.5941 - val_accuracy: 0.8209\n",
      "Epoch 21/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0273 - accuracy: 0.9872 - val_loss: 11.0192 - val_accuracy: 0.4349\n",
      "Epoch 22/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0251 - accuracy: 0.9882 - val_loss: 11.9678 - val_accuracy: 0.4349\n",
      "Epoch 23/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0274 - accuracy: 0.9871 - val_loss: 11.4033 - val_accuracy: 0.4349\n",
      "Epoch 24/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0243 - accuracy: 0.9888 - val_loss: 5.7237 - val_accuracy: 0.5693\n",
      "Epoch 25/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0252 - accuracy: 0.9877 - val_loss: 13.8307 - val_accuracy: 0.4349\n",
      "Epoch 26/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0395 - accuracy: 0.9819 - val_loss: 16.7065 - val_accuracy: 0.4349\n",
      "Epoch 27/30\n",
      "60/60 [==============================] - 5s 78ms/step - loss: 0.0247 - accuracy: 0.9889 - val_loss: 13.9868 - val_accuracy: 0.4349\n",
      "Epoch 28/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0262 - accuracy: 0.9878 - val_loss: 12.6274 - val_accuracy: 0.4349\n",
      "Epoch 29/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0250 - accuracy: 0.9884 - val_loss: 15.5367 - val_accuracy: 0.4349\n",
      "Epoch 30/30\n",
      "60/60 [==============================] - 5s 77ms/step - loss: 0.0237 - accuracy: 0.9892 - val_loss: 6.5621 - val_accuracy: 0.5066\n"
     ]
    }
   ],
   "source": [
    "model = create_model()\n",
    "model.compile(optimizer='Adam', loss = 'binary_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(input_train, y_train, validation_data=(input_val, y_val), epochs = 30, batch_size=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.13%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(input_test, y_test, batch_size = 2048, verbose = 0)\n",
    "print(str(int(accuracy*10000)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = testingdataset\n",
    "y_true = dataset_test['label']\n",
    "y_true = y_true.to_list()\n",
    "scaler = MinMaxScaler()\n",
    "df = dataset_test.drop(columns=['label'])\n",
    "X = scaler.fit_transform(df)\n",
    "y = model.predict(X.reshape(666,76,1))\n",
    "y_round = tf.make_ndarray(tf.make_tensor_proto(tf.round(y))).reshape([len(y)])\n",
    "y_round = list(y_round)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gebalanceerde accuracy van model met een ongekende dataset:\n",
      "50.0%\n"
     ]
    }
   ],
   "source": [
    "print(\"Gebalanceerde accuracy van model met een ongekende dataset:\")\n",
    "print(str(int(balanced_accuracy_score(y_true, y_round)*10000)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gebalanceerde accuracy van model met een ongekende dataset:\n",
      "32.83%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "print(\"Gebalanceerde accuracy van model met een ongekende dataset:\")\n",
    "print(str(int(f1_score(y_true, y_round, average='weighted')*10000)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
