{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyarrow\n",
      "  Downloading pyarrow-2.0.0-cp37-cp37m-manylinux2014_x86_64.whl (17.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 17.7 MB 7.1 MB/s eta 0:00:01███████████▌      | 14.1 MB 7.1 MB/s eta 0:00:01ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /opt/conda/lib/python3.7/site-packages (from pyarrow) (1.18.5)\n",
      "Installing collected packages: pyarrow\n",
      "Successfully installed pyarrow-2.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q sklearn\n",
    "!pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import feature_column\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "PREPATH = \"/project/Alexander/Datasets_RAW\"\n",
    "df2017 = [\n",
    "    \"/Friday-WorkingHours-Afternoon-DDos.pcap_ISCX.csv\",\n",
    "    \"/Friday-WorkingHours-Afternoon-PortScan.pcap_ISCX.csv\",\n",
    "    \"/Friday-WorkingHours-Morning.pcap_ISCX.csv\",\n",
    "    \"/Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"/Thursday-WorkingHours-Afternoon-Infilteration.pcap_ISCX.csv\",\n",
    "    \"/Thursday-WorkingHours-Morning-WebAttacks.pcap_ISCX.csv\",\n",
    "    \"/Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"/Wednesday-workingHours.pcap_ISCX.csv\"\n",
    "]\n",
    "dfs_names = [df2017]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "i = 0\n",
    "for dataset in dfs_names:\n",
    "    for path in dataset:\n",
    "        dfs[i] = pd.read_csv(f'{PREPATH}{path}', low_memory=False, header=0, names= [\"Flow ID\", \"Source IP\", \"Source Port\", \"Destination IP\",\"destination port\", \"protocol\", \"timestamp\", \"flow duration\", \"total fwd packets\", \"total backward packets\", \"total length of fwd packets\", \"total length of bwd packets\", \"fwd packet length max\", \"fwd packet length min\", \"fwd packet length mean\", \"fwd packet length std\", \"bwd packet length max\", \"bwd packet length min\", \"bwd packet length mean\", \"bwd packet length std\", \"flow bytes/s\", \"flow packets/s\", \"flow iat mean\", \"flow iat std\", \"flow iat max\", \"flow iat min\", \"fwd iat total\", \"fwd iat mean\", \"fwd iat std\", \"fwd iat max\", \"fwd iat min\", \"bwd iat total\", \"bwd iat mean\", \"bwd iat std\", \"bwd iat max\", \"bwd iat min\", \"fwd psh flags\", \"bwd psh flags\", \"fwd urg flags\", \"bwd urg flags\", \"fwd header length\", \"bwd header length\", \"fwd packets/s\", \"bwd packets/s\", \"min packet length\", \"max packet length\", \"packet length mean\", \"packet length std\", \"packet length variance\", \"fin flag count\", \"syn flag count\", \"rst flag count\", \"psh flag count\", \"ack flag count\", \"urg flag count\", \"cwe flag count\", \"ece flag count\", \"down/up ratio\", \"average packet size\", \"avg fwd segment size\", \"avg bwd segment size\", \"fwd avg bytes/bulk\", \"fwd avg packets/bulk\", \"fwd avg bulk rate\", \"bwd avg bytes/bulk\", \"bwd avg packets/bulk\", \"bwd avg bulk rate\", \"subflow fwd packets\", \"subflow fwd bytes\", \"subflow bwd packets\", \"subflow bwd bytes\", \"init_win_bytes_forward\", \"init_win_bytes_backward\", \"act_data_pkt_fwd\", \"min_seg_size_forward\", \"active mean\", \"active std\", \"active max\", \"active min\", \"idle mean\", \"idle std\", \"idle max\", \"idle min\", \"label\"])\n",
    "        dfs[i] = dfs[i][[\"Source IP\", \"Source Port\", \"Destination IP\",\"destination port\", \"protocol\", \"timestamp\", \"flow duration\", \"fin flag count\", \"syn flag count\", \"rst flag count\", \"psh flag count\", \"ack flag count\", \"urg flag count\",\"total fwd packets\", \"total backward packets\",\"total length of fwd packets\", \"total length of bwd packets\",\"label\"]]\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "regex_patDOS = re.compile(r'.*dos.*', flags=re.IGNORECASE)\n",
    "regex_patBruteForce = re.compile(r'.*brute.*', flags=re.IGNORECASE)\n",
    "for i, path in dfs.items():\n",
    "    dfs[i] = dfs[i].replace(['slowloris','hulk','Bot','slowbody2','slowread','goldeneye','slowheaders','rudy','TFTP','Syn','UDP-lag','Portmap','UDP','UDPLag'], \"DoS\")\n",
    "    dfs[i] = dfs[i].replace(regex_patBruteForce, \"BruteForce\")\n",
    "    dfs[i] = dfs[i].replace(regex_patDOS, \"DoS\")\n",
    "    dfs[i] = dfs[i].replace(['SQL Injection','NetBIOS','LDAP','MSSQL','Heartbleed','FTP-Patator','SSH-Patator'], \"BruteForce\")\n",
    "    dfs[i] = dfs[i].replace(['.*XSS.*','.*Sql.*'], \"DoS\",regex=True)\n",
    "    dfs[i] = dfs[i].replace(['BENIGN','Benign'], \"normal\")\n",
    "    dfs[i] = dfs[i][dfs[i].label != 'Infiltration']\n",
    "    dfs[i] = dfs[i][dfs[i].label != 'Infilteration']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for key, path in dfs.items():\n",
    "    dfs[key]['flags'] = \"\"\n",
    "    dfs[key]['flags'] = np.where(dfs[key]['fin flag count'] > 0 , dfs[key].flags + \"FIN \", dfs[key].flags)\n",
    "    dfs[key]['flags'] = np.where(dfs[key]['syn flag count'] > 0 , dfs[key].flags + \"SYN \", dfs[key].flags)\n",
    "    dfs[key]['flags'] = np.where(dfs[key]['rst flag count'] > 0 , dfs[key].flags + \"RST \", dfs[key].flags)\n",
    "    dfs[key]['flags'] = np.where(dfs[key]['psh flag count'] > 0 , dfs[key].flags + \"PSH \", dfs[key].flags)\n",
    "    dfs[key]['flags'] = np.where(dfs[key]['ack flag count'] > 0 , dfs[key].flags + \"ACK \", dfs[key].flags)\n",
    "    dfs[key]['flags'] = np.where(dfs[key]['urg flag count'] > 0 , dfs[key].flags + \"URG \", dfs[key].flags)\n",
    "    dfs[key] = dfs[key].drop(columns = ['fin flag count', 'syn flag count', 'rst flag count', 'psh flag count', 'ack flag count','urg flag count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(props):\n",
    "    start_mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    NAlist = [] # Keeps track of columns that have missing values filled in. \n",
    "    for col in props.columns:\n",
    "        if props[col].dtype != object and props[col].dtype != 'datetime64[ns]':  # Exclude strings\n",
    "            \n",
    "            # make variables for Int, max and min\n",
    "            IsInt = False\n",
    "            mx = props[col].max()\n",
    "            mn = props[col].min()\n",
    "            \n",
    "            # Integer does not support NA, therefore, NA needs to be filled\n",
    "            if not np.isfinite(props[col]).all(): \n",
    "                NAlist.append(col)\n",
    "                props[col].fillna(mn-1,inplace=True)  \n",
    "                   \n",
    "            # test if column can be converted to an integer\n",
    "            asint = props[col].fillna(0).astype(np.int64)\n",
    "            result = (props[col] - asint)\n",
    "            result = result.sum()\n",
    "            if result > -0.01 and result < 0.01:\n",
    "                IsInt = True\n",
    "\n",
    "            \n",
    "            # Make Integer/unsigned Integer datatypes\n",
    "            if IsInt:\n",
    "                if mn >= 0:\n",
    "                    if mx < 255:\n",
    "                        props[col] = props[col].astype(np.uint8)\n",
    "                    elif mx < 65535:\n",
    "                        props[col] = props[col].astype(np.uint16)\n",
    "                    elif mx < 4294967295:\n",
    "                        props[col] = props[col].astype(np.uint32)\n",
    "                    else:\n",
    "                        props[col] = props[col].astype(np.uint64)\n",
    "                else:\n",
    "                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n",
    "                        props[col] = props[col].astype(np.int8)\n",
    "                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n",
    "                        props[col] = props[col].astype(np.int16)\n",
    "                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n",
    "                        props[col] = props[col].astype(np.int32)\n",
    "                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n",
    "                        props[col] = props[col].astype(np.int64)    \n",
    "            \n",
    "            # Make float datatypes 32 bit\n",
    "            else:\n",
    "                props[col] = props[col].astype(np.float32)\n",
    "    mem_usg = props.memory_usage().sum() / 1024**2 \n",
    "    return props, NAlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, path in dfs.items():\n",
    "    dfs[i]['packets'] =  dfs[i][\"total fwd packets\"] +  dfs[i][\"total backward packets\"]\n",
    "    dfs[i]['bytes'] =  dfs[i]['total length of fwd packets'] +  dfs[i]['total length of bwd packets']\n",
    "    dfs[i], NAlist = reduce_mem_usage(dfs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['BruteForce' 'DoS' 'PortScan' 'normal']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "attack_types = []\n",
    "for i, path in dfs.items():\n",
    "    attack_types = list(dfs[i][\"label\"].unique()) + attack_types\n",
    "u, indices = np.unique(attack_types, return_inverse=True)\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipaddress\n",
    "for i, path in dfs.items():\n",
    "    dfs[i]['timestamp'] = dfs[i]['timestamp'].astype(dtype='datetime64[ns]')\n",
    "    dfs[i]['timestamp'] = dfs[i]['timestamp'].astype(int)\n",
    "    dfs[i]['Source IP'] = int(ipaddress.ip_address(dfs[i]['Source IP'][1]))\n",
    "    dfs[i]['Destination IP'] = int(ipaddress.ip_address(dfs[i]['Destination IP'][1]))\n",
    "    dfs[i] = dfs[i].drop(columns = ['total fwd packets', 'total backward packets', 'total length of fwd packets', 'total length of bwd packets'])\n",
    "    dfs[i] = dfs[i].replace(['BruteForce','DoS','PortScan'], \"attack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Source IP  Source Port  Destination IP  destination port  protocol  \\\n",
      "0       3232238085        49182      3232238083                88         6   \n",
      "1       3232238085        49183      3232238083                88         6   \n",
      "2       3232238085        49184      3232238083                88         6   \n",
      "3       3232238085        49185      3232238083                88         6   \n",
      "4       3232238085        49186      3232238083                88         6   \n",
      "...            ...          ...             ...               ...       ...   \n",
      "445904  3232238085        51694      3232238083                53        17   \n",
      "445905  3232238085          443      3232238083             59317         6   \n",
      "445906  3232238085        62256      3232238083                53        17   \n",
      "445907  3232238085          443      3232238083             54726         6   \n",
      "445908  3232238085        51801      3232238083                53        17   \n",
      "\n",
      "                  timestamp  flow duration  fin flag count  syn flag count  \\\n",
      "0       1491555240000000000            640               0               0   \n",
      "1       1491555240000000000            900               0               0   \n",
      "2       1491555240000000000           1205               0               0   \n",
      "3       1491555240000000000            511               0               0   \n",
      "4       1491555240000000000            773               0               0   \n",
      "...                     ...            ...             ...             ...   \n",
      "445904  1491559740000000000            155               0               0   \n",
      "445905  1491540060000000000            110               0               0   \n",
      "445906  1491556260000000000            166               0               0   \n",
      "445907  1491569280000000000             81               0               0   \n",
      "445908  1491561480000000000            202               0               0   \n",
      "\n",
      "        rst flag count  psh flag count  ack flag count  urg flag count  \\\n",
      "0                    0               1               0               0   \n",
      "1                    0               1               0               0   \n",
      "2                    0               1               0               0   \n",
      "3                    0               1               0               0   \n",
      "4                    0               1               0               0   \n",
      "...                ...             ...             ...             ...   \n",
      "445904               0               0               0               0   \n",
      "445905               0               0               1               1   \n",
      "445906               0               0               0               0   \n",
      "445907               0               0               1               0   \n",
      "445908               0               0               0               0   \n",
      "\n",
      "         label  packets  bytes  \n",
      "0       normal       11    798  \n",
      "1       normal       13   3544  \n",
      "2       normal       11   5606  \n",
      "3       normal       11    822  \n",
      "4       normal       13   3556  \n",
      "...        ...      ...    ...  \n",
      "445904  normal        4    208  \n",
      "445905  normal        2      0  \n",
      "445906  normal        4    276  \n",
      "445907  normal        2      0  \n",
      "445908  normal        4    260  \n",
      "\n",
      "[445909 rows x 16 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-3988600c2f62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Datasets_RAW/Feather/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mi\u001b[0m\u001b[0;34m+=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "teller = 0\n",
    "i = 0\n",
    "for dataset in dfs_names:\n",
    "    for path in dataset:\n",
    "        dfs[i] = dfs[i].reset_index(drop=True)\n",
    "        dfs[i].to_feather('./Datasets_RAW/Feather/'+path.split('/')[2].split('.')[0])\n",
    "        i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] Failed to open local file './Datasets_RAW/Feather/Friday-WorkingHours-Afternoon-DDos'. Detail: [errno 2] No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-6a6e127e8da8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdfs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Datasets_RAW/Feather'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/io/feather_format.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(path, columns, use_threads)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstringify_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfeather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_feather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mread_feather\u001b[0;34m(source, columns, use_threads, memory_map)\u001b[0m\n\u001b[1;32m    213\u001b[0m     \"\"\"\n\u001b[1;32m    214\u001b[0m     \u001b[0m_check_pandas_version\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 215\u001b[0;31m     return (read_table(source, columns=columns, memory_map=memory_map)\n\u001b[0m\u001b[1;32m    216\u001b[0m             .to_pandas(use_threads=use_threads))\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/feather.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, memory_map)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \"\"\"\n\u001b[1;32m    236\u001b[0m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFeatherReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m     \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_memory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemory_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/feather.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.FeatherReader.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_reader\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.get_native_file\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.memory_map\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/io.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.MemoryMappedFile._open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.pyarrow_internal_check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pyarrow/error.pxi\u001b[0m in \u001b[0;36mpyarrow.lib.check_status\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] Failed to open local file './Datasets_RAW/Feather/Friday-WorkingHours-Afternoon-DDos'. Detail: [errno 2] No such file or directory"
     ]
    }
   ],
   "source": [
    "dfs = {}\n",
    "i = 0 \n",
    "for dataset in dfs_names:\n",
    "    for path in dataset:\n",
    "        index = path.split('.')[0]\n",
    "        #dfs[i] = pd.read_feather('./Datasets_RAW/Feather'+ index)\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "\n",
    "dataframe = dfs[6]\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "dataframe['label'] = label_encoder.fit_transform(dataframe['label']) \n",
    "dataframe['target'] = dataframe['label']\n",
    "dataframe = dataframe.drop(columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 0]\n"
     ]
    }
   ],
   "source": [
    "print(dataframe['target'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Source_IP', 'Source_Port', 'Destination_IP', 'destination_port',\n",
      "       'protocol', 'timestamp', 'flow_duration', 'fin_flag_count',\n",
      "       'syn_flag_count', 'rst_flag_count', 'psh_flag_count', 'ack_flag_count',\n",
      "       'urg_flag_count', 'packets', 'bytes', 'target'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataframe.columns = dataframe.columns.str.replace(' ', '_')\n",
    "print(dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "298759 train examples\n",
      "147150 test examples\n"
     ]
    }
   ],
   "source": [
    "train, test = train_test_split(dataframe, test_size=0.33)\n",
    "print(len(train), 'train examples')\n",
    "print(len(test), 'test examples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility method to create a tf.data dataset from a Pandas Dataframe\n",
    "def df_to_dataset(dataframe, shuffle=True, batch_size=500):\n",
    "    dataframe = dataframe.copy()\n",
    "    labels = dataframe.pop('target')\n",
    "    print(labels)\n",
    "    ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = []\n",
    "\n",
    "# numeric cols\n",
    "for header in dataframe.columns:\n",
    "    if header!='target':\n",
    "        feature_columns.append(feature_column.numeric_column(header))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "209624    1\n",
      "429012    1\n",
      "291688    1\n",
      "101792    1\n",
      "411550    1\n",
      "         ..\n",
      "55149     1\n",
      "12474     1\n",
      "91193     1\n",
      "213296    1\n",
      "206394    1\n",
      "Name: target, Length: 298759, dtype: int64\n",
      "138561    1\n",
      "94415     1\n",
      "12331     1\n",
      "344348    1\n",
      "179184    1\n",
      "         ..\n",
      "175805    1\n",
      "356089    1\n",
      "42247     1\n",
      "313136    1\n",
      "122764    1\n",
      "Name: target, Length: 147150, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "batch_size = 500\n",
    "train_ds = df_to_dataset(train, batch_size=batch_size)\n",
    "test_ds = df_to_dataset(test, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "598/598 [==============================] - 19s 32ms/step - loss: 1.1551e-07 - accuracy: 0.9689 - val_loss: 1.1552e-07 - val_accuracy: 0.9690\n",
      "Epoch 2/5\n",
      "598/598 [==============================] - 19s 32ms/step - loss: 1.1551e-07 - accuracy: 0.9689 - val_loss: 1.1552e-07 - val_accuracy: 0.9690\n",
      "Epoch 3/5\n",
      "598/598 [==============================] - 26s 43ms/step - loss: 1.1551e-07 - accuracy: 0.9689 - val_loss: 1.1552e-07 - val_accuracy: 0.9690\n",
      "Epoch 4/5\n",
      "598/598 [==============================] - 16s 26ms/step - loss: 1.1551e-07 - accuracy: 0.9689 - val_loss: 1.1552e-07 - val_accuracy: 0.9690\n",
      "Epoch 5/5\n",
      "598/598 [==============================] - 15s 25ms/step - loss: 1.1551e-07 - accuracy: 0.9689 - val_loss: 1.1552e-07 - val_accuracy: 0.9690\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f7bf463f790>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow import expand_dims\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.DenseFeatures(feature_columns),\n",
    "    tf.keras.layers.Reshape((-1, 15), input_shape=(15,)),\n",
    "    layers.LSTM(6, return_sequences = True),\n",
    "    layers.Dense(1, activation= 'softmax')\n",
    "])\n",
    "\n",
    "opt = tf.keras.optimizers.RMSprop(learning_rate=0.01, rho=0.05, momentum=0.0, epsilon=1e-08, centered=False, name='RMSprop')\n",
    "model.compile(optimizer=opt, loss = 'categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(train_ds, validation_data=test_ds, epochs=5, batch_size=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\n        outputs = self.distribute_strategy.run(\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\n        return self(x, training=False)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:291 call\n        outputs = layer(inputs, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/dense_features.py:139 call  **\n        features)\n\n    ValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'IteratorGetNext:0' shape=(None, 15) dtype=int64>)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-5feb773233ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'target'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0my_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_ndarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_tensor_proto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mround\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0my_round\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_round\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m       raise ValueError('{} is not supported in multi-worker mode.'.format(\n\u001b[1;32m     87\u001b[0m           method.__name__))\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m   return tf_decorator.make_decorator(\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1266\u001b[0m           \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1267\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_predict_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1268\u001b[0;31m             \u001b[0mtmp_batch_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1269\u001b[0m             \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1270\u001b[0m             \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    625\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    628\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    504\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    505\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 506\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2444\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2446\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2447\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2448\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2775\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2776\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2777\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2778\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2779\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2665\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2666\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2667\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2668\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2669\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    979\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    982\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0;31m# __wrapped__ allows AutoGraph to swap in a converted function. We give\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 441\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    442\u001b[0m     \u001b[0mweak_wrapped_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweakref\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    966\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 968\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    969\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1147 predict_function  *\n        outputs = self.distribute_strategy.run(\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:951 run  **\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2290 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/distribute/distribute_lib.py:2649 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py:1122 predict_step  **\n        return self(x, training=False)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/sequential.py:291 call\n        outputs = layer(inputs, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/keras/engine/base_layer.py:927 __call__\n        outputs = call_fn(cast_inputs, *args, **kwargs)\n    /opt/conda/lib/python3.7/site-packages/tensorflow/python/feature_column/dense_features.py:139 call  **\n        features)\n\n    ValueError: ('We expected a dictionary here. Instead we got: ', <tf.Tensor 'IteratorGetNext:0' shape=(None, 15) dtype=int64>)\n"
     ]
    }
   ],
   "source": [
    "y_true = test['target']\n",
    "y_true = y_true.to_list()\n",
    "X = test.drop(columns=['target'])\n",
    "y = model.predict(X)\n",
    "y_round = tf.make_ndarray(tf.make_tensor_proto(tf.round(y))).reshape([len(y)])\n",
    "y_round = list(y_round)\n",
    "print(\"Gebalanceerde accuracy van model met een ongekende dataset:\")\n",
    "print(str(int(balanced_accuracy_score(y_true, y_round)*10000)/100) + \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('./checkpoints/checkpoint')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install eli5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import eli5\n",
    "from eli5.sklearn import PermutationImportance\n",
    "#df = dfs[6]['destination port', 'protocol', 'timestamp', 'flow duration','fin flag count', 'syn flag count', 'rst flag count', 'psh flag count','ack flag count', 'urg flag count', 'packets', 'bytes', 'target']\n",
    "#res = dfs[6]['target']\n",
    "perm = PermutationImportance(model,scoring=\"accuracy\", random_state=1).fit(train_ds, test_ds)\n",
    "eli5.show_weights(perm, feature_names = dataframe.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Source IP  Source Port  Destination IP  destination port  protocol  \\\n",
      "0       3232238085        49182      3232238083                88         6   \n",
      "1       3232238085        49183      3232238083                88         6   \n",
      "2       3232238085        49184      3232238083                88         6   \n",
      "3       3232238085        49185      3232238083                88         6   \n",
      "4       3232238085        49186      3232238083                88         6   \n",
      "...            ...          ...             ...               ...       ...   \n",
      "445904  3232238085        51694      3232238083                53        17   \n",
      "445905  3232238085          443      3232238083             59317         6   \n",
      "445906  3232238085        62256      3232238083                53        17   \n",
      "445907  3232238085          443      3232238083             54726         6   \n",
      "445908  3232238085        51801      3232238083                53        17   \n",
      "\n",
      "                  timestamp  flow duration  fin flag count  syn flag count  \\\n",
      "0       1491555240000000000            640               0               0   \n",
      "1       1491555240000000000            900               0               0   \n",
      "2       1491555240000000000           1205               0               0   \n",
      "3       1491555240000000000            511               0               0   \n",
      "4       1491555240000000000            773               0               0   \n",
      "...                     ...            ...             ...             ...   \n",
      "445904  1491559740000000000            155               0               0   \n",
      "445905  1491540060000000000            110               0               0   \n",
      "445906  1491556260000000000            166               0               0   \n",
      "445907  1491569280000000000             81               0               0   \n",
      "445908  1491561480000000000            202               0               0   \n",
      "\n",
      "        rst flag count  psh flag count  ack flag count  urg flag count  label  \\\n",
      "0                    0               1               0               0      1   \n",
      "1                    0               1               0               0      1   \n",
      "2                    0               1               0               0      1   \n",
      "3                    0               1               0               0      1   \n",
      "4                    0               1               0               0      1   \n",
      "...                ...             ...             ...             ...    ...   \n",
      "445904               0               0               0               0      1   \n",
      "445905               0               0               1               1      1   \n",
      "445906               0               0               0               0      1   \n",
      "445907               0               0               1               0      1   \n",
      "445908               0               0               0               0      1   \n",
      "\n",
      "        packets  bytes  target  \n",
      "0            11    798       1  \n",
      "1            13   3544       1  \n",
      "2            11   5606       1  \n",
      "3            11    822       1  \n",
      "4            13   3556       1  \n",
      "...         ...    ...     ...  \n",
      "445904        4    208       1  \n",
      "445905        2      0       1  \n",
      "445906        4    276       1  \n",
      "445907        2      0       1  \n",
      "445908        4    260       1  \n",
      "\n",
      "[445909 rows x 17 columns]\n"
     ]
    }
   ],
   "source": [
    "print(dfs[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
